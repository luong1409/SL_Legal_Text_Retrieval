{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/huy/anaconda3/envs/ltr/lib/python3.7/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "import json\n",
    "import pickle\n",
    "\n",
    "from transformers.tokenization_utils import PreTrainedTokenizer\n",
    "from data_generator import vi_tokenize\n",
    "\n",
    "from tfidf_classifier import do_classify\n",
    "from transformers import AutoConfig, AutoModelForSequenceClassification, AutoTokenizer\n",
    "from transformers import GlueDataTrainingArguments as DataTrainingArguments\n",
    "from transformers import (\n",
    "    HfArgumentParser,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    ")\n",
    "\n",
    "from transformers.data.datasets.glue import *\n",
    "from transformers.data.processors.utils import InputExample\n",
    "\n",
    "from utils import Question, load_data_kse, standardize_data, Article\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class ModelArguments:\n",
    "    \"\"\"\n",
    "    Arguments pertaining to which model/config/tokenizer we are going to fine-tune from.\n",
    "    \"\"\"\n",
    "    model_name_or_path: str = field(\n",
    "        metadata={\n",
    "            \"help\": \"Path to pretrained model or model identifier from huggingface.co/models\"}\n",
    "    )\n",
    "    config_name: Optional[str] = field(\n",
    "        default=None, metadata={\"help\": \"Pretrained config name or path if not the same as model_name\"}\n",
    "    )\n",
    "    tokenizer_name: Optional[str] = field(\n",
    "        default=None, metadata={\"help\": \"Pretrained tokenizer name or path if not the same as model_name\"}\n",
    "    )\n",
    "    cache_dir: Optional[str] = field(\n",
    "        default=None, metadata={\"help\": \"Where do you want to store the pretrained models downloaded from s3\"}\n",
    "    )\n",
    "\n",
    "\n",
    "class LawDataset(Dataset):\n",
    "    \"\"\"\n",
    "    This will be superseded by a framework-agnostic approach\n",
    "    soon.\n",
    "    \"\"\"\n",
    "    args: GlueDataTrainingArguments\n",
    "    output_mode: str\n",
    "    features: List[InputFeatures]\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            args: GlueDataTrainingArguments,\n",
    "            tokenizer: PreTrainedTokenizer,\n",
    "            limit_examples: Optional[int] = None,\n",
    "            mode: Union[str, Split] = Split.train,\n",
    "            c_code=None,\n",
    "            sentence=None,\n",
    "    ):\n",
    "        self.args = args\n",
    "        task_name = 'mrpc'\n",
    "        self.processor = glue_processors[task_name]()\n",
    "        self.output_mode = 'classification'\n",
    "        self.c_code = c_code if c_code is not None else []\n",
    "        self.sentence = sentence if sentence is not None else \"\"\n",
    "        if isinstance(mode, str):\n",
    "            try:\n",
    "                mode = Split[mode]\n",
    "            except KeyError:\n",
    "                raise KeyError(\"mode is not a valid split name\")\n",
    "\n",
    "        label_list = self.processor.get_labels()\n",
    "        self.label_list = label_list\n",
    "\n",
    "        def _create_examples(lines, set_type='test'):\n",
    "            examples = []\n",
    "            for (i, line) in enumerate(lines):\n",
    "                guid = \"%s-%s\" % (set_type, i)\n",
    "                text_a = line[3]\n",
    "                text_b = line[4]\n",
    "                label = None if set_type == \"test\" else line[0]\n",
    "                examples.append(InputExample(\n",
    "                    guid=guid, text_a=text_a, text_b=text_b, label=label))\n",
    "            return examples\n",
    "\n",
    "        lines = []\n",
    "        for i, e in enumerate(self.c_code):\n",
    "            lines.append([0, \"sent_{}\".format(\n",
    "                i), e[1], self.sentence[i], e[0]])\n",
    "\n",
    "        # recreate the data\n",
    "        examples = _create_examples(lines)\n",
    "        if limit_examples is not None:\n",
    "            examples = examples[:limit_examples]\n",
    "        self.features = glue_convert_examples_to_features(\n",
    "            examples,\n",
    "            tokenizer,\n",
    "            max_length=args.max_seq_length,\n",
    "            label_list=label_list,\n",
    "            output_mode=self.output_mode,\n",
    "        )\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "\n",
    "    def __getitem__(self, i) -> InputFeatures:\n",
    "        return self.features[i]\n",
    "\n",
    "    def get_labels(self):\n",
    "        return self.label_list\n",
    "\n",
    "    def get_c_code_ids(self):\n",
    "        return [e[1] for e in self.c_code]\n",
    "\n",
    "\n",
    "def infer_coliee_task3(\n",
    "    sentence, \n",
    "    all_civil_code, \n",
    "    data_args, \n",
    "    tfidf_vectorizer, \n",
    "    trainer, \n",
    "    bert_tokenizer, \n",
    "    tokenizer=None, \n",
    "    topk=150\n",
    "):\n",
    "    if isinstance(sentence, str):\n",
    "        sentence = [sentence]\n",
    "    test_q = [Question(id='q{}'.format(i), content=tokenizer(\n",
    "        s)if tokenizer is not None else s, content_raw=s, relevant_a=[]) for i, s in enumerate(sentence)]\n",
    "    c_docs = all_civil_code[0]\n",
    "    c_keys = all_civil_code[1]\n",
    "    c_vect = all_civil_code[2]\n",
    "    c_docs_keys = list(zip(all_civil_code[0], all_civil_code[1]))\n",
    "\n",
    "    test_pred, _ = do_classify(\n",
    "        c_docs, c_keys, test_q, vectorizer=tfidf_vectorizer, topk=topk, c_vect=c_vect,  combine_score=(tokenizer is None))\n",
    "\n",
    "    c_code_pred_by_tfidf = []\n",
    "    coressponding_questions = []\n",
    "    for i, s_pred in enumerate(test_pred):\n",
    "        for idx in s_pred:\n",
    "            coressponding_questions.append(test_q[i].content)\n",
    "            c_code_pred_by_tfidf.append(c_docs_keys[idx])\n",
    "    \n",
    "    test_dataset = LawDataset(\n",
    "        data_args,\n",
    "        bert_tokenizer,\n",
    "        mode='test', \n",
    "        sentence=coressponding_questions, \n",
    "        c_code=c_code_pred_by_tfidf\n",
    "    )\n",
    "    \n",
    "    predictions = trainer.predict(test_dataset=test_dataset).predictions\n",
    "    probs = torch.softmax(torch.from_numpy(predictions), dim=1)\n",
    "    predicted_labels = torch.argmax(probs, 1)\n",
    "    return predicted_labels, probs, c_code_pred_by_tfidf\n",
    "\n",
    "def list_split(listA, n):\n",
    "    for x in range(0, len(listA), n):\n",
    "        every_chunk = listA[x: n+x]\n",
    "\n",
    "        if len(every_chunk) < n:\n",
    "            every_chunk = every_chunk + \\\n",
    "                [None for y in range(n-len(every_chunk))]\n",
    "        yield every_chunk\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_state(\n",
    "    path_c_code, \n",
    "    path_data_org, \n",
    "    path_preprocessed_data, \n",
    "    model_path, \n",
    "    tokenizer=None, \n",
    "    topk=150, \n",
    "    testing_data=None, \n",
    "    max_seq_length=512,\n",
    "    do_lower_case=True\n",
    "):\n",
    "    \n",
    "    model_version = model_path  # 'bert-base-uncased'\n",
    "    \n",
    "    config = AutoConfig.from_pretrained(\n",
    "        model_version,\n",
    "        num_labels=2,\n",
    "        finetuning_task='MRPC'\n",
    "    )\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        model_version, config=config)\n",
    "    bert_tokenizer = AutoTokenizer.from_pretrained(\n",
    "        model_version, do_lower_case=do_lower_case)\n",
    "    model.eval()\n",
    "    \n",
    "    parser = HfArgumentParser(\n",
    "        (ModelArguments, DataTrainingArguments, TrainingArguments))\n",
    "    model_args, data_args, training_args = parser.parse_args_into_dataclasses(\n",
    "        args=[\n",
    "            \"--model_name_or_path\", model_version,\n",
    "            \"--task_name\", \"MRPC\",\n",
    "            \"--data_dir\", \"./coliee3_2020/data\",\n",
    "            \"--do_predict\",\n",
    "            \"--per_device_train_batch_size\", \"16\",\n",
    "            \"--max_seq_length\", \"{}\".format(max_seq_length),\n",
    "            \"--learning_rate\", \"2e-5\",\n",
    "            \"--output_dir\", model_version,\n",
    "            \"--overwrite_output_dir\"\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    # Initialize our Trainer\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args\n",
    "    )\n",
    "    \n",
    "    tfidf_vectorizer = pickle.load(\n",
    "        open(\"{}/tfidf_classifier.pkl\".format(path_preprocessed_data), \"rb\"))\n",
    "    if isinstance(tfidf_vectorizer, tuple):\n",
    "        tfidf_vectorizer, bm25_scorer = tfidf_vectorizer[0], tfidf_vectorizer[1]\n",
    "    \n",
    "    path_data_cached = '{}/tokenized_data_cached.pkl'.format(path_preprocessed_data)\n",
    "    \n",
    "    if os.path.isfile(path_data_cached):\n",
    "        print(\"Load cached file data: {}\".format(path_data_cached))\n",
    "        c_docs, c_keys, dev_q, test_q, train_q, sub_doc_info = pickle.load(open(path_data_cached, 'rb'))\n",
    "        \n",
    "    else:\n",
    "        print(\"Load data and tokenize data\")\n",
    "        c_docs, c_keys, dev_q, test_q, train_q, sub_doc_info = load_data_kse(\n",
    "            path_folder_base=path_data_org,  ids_test=[\n",
    "            ], tokenizer=tokenizer, testing_data=testing_data,\n",
    "            # chunk_content_info=chunk_content_info\n",
    "        )\n",
    "        \n",
    "    c_vect = tfidf_vectorizer.transform([standardize_data(d) for d in c_docs])\n",
    "    return (c_docs, c_keys, c_vect), data_args, (tfidf_vectorizer, bm25_scorer), trainer, bert_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"PhoBERT\": {\n",
      "    \"path_data_org\": \"../data/zac2021-ltr-data/\",\n",
      "    \"path_c_code\": \"../data/zac2021-ltr-data/new_corpus.json\",\n",
      "    \"tokenizer\": \"vi_tokenize\",\n",
      "    \"topk\": 300,\n",
      "    \"do_lower_case\": true,\n",
      "    \"max_seq_length\": 256,\n",
      "    \"path_preprocessed_data\": \"../data/zalo-tfidfbm25150-full/\",\n",
      "    \"model_path\": \"../settings/Tfbm150E5-full42/models\"\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "global all_civil_code, data_args, tfidf_vectorizer, trainer, bert_tokenizer\n",
    "\n",
    "model_configs = {\n",
    "    'PhoBERT': {\n",
    "        \"path_data_org\": '../data/zac2021-ltr-data/',\n",
    "        \"path_c_code\": '../data/zac2021-ltr-data/new_corpus.json',\n",
    "        \"tokenizer\": 'vi_tokenize',\n",
    "        \"topk\": 300,\n",
    "        \"do_lower_case\": True,\n",
    "        \"max_seq_length\": 256,\n",
    "        \"path_preprocessed_data\": '../data/zalo-tfidfbm25150-full/',\n",
    "        \"model_path\": '../settings/Tfbm150E5-full42/models',\n",
    "    }\n",
    "}\n",
    "print(json.dumps(model_configs, indent=2))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_all_data = json.load(open('../data/zac2021-ltr-data/test.json'))['items']\n",
    "test_ids = [e['question_id'] for e in test_all_data]\n",
    "test_sents = [e['question'] for e in test_all_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model ....\n",
      "Load cached file data: ../data/zalo-tfidfbm25150-full//tokenized_data_cached.pkl\n",
      "Finish loaded model\n"
     ]
    }
   ],
   "source": [
    "model_init_states = {}\n",
    "print(\"Loading model ....\")\n",
    "for m_name, model_info in model_configs.items():\n",
    "    if 'tokenizer' in model_info and model_info['tokenizer'] == 'vi_tokenize':\n",
    "        model_info['tokenizer'] = vi_tokenize\n",
    "        \n",
    "    model_init_states[m_name] = init_state(**model_info)\n",
    "    \n",
    "    all_civil_code, data_args, tfidf_vectorizer, trainer, bert_tokenizer = model_init_states[m_name]\n",
    "    \n",
    "    tokenizer = model_info.get('tokenizer')\n",
    "    topk = 150\n",
    "\n",
    "print(\"Finish loaded model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GlueDataTrainingArguments(task_name='mrpc', data_dir='./coliee3_2020/data', max_seq_length=256, overwrite_cache=False)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset = LawDataset(\n",
    "    data_args,\n",
    "    bert_tokenizer,\n",
    "    mode='test', \n",
    "    sentence=coressponding_questions, \n",
    "    c_code=c_code_pred_by_tfidf\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_prediction = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start infer\n",
    "time_start = time.time()\n",
    "for m_name, model_info in model_configs.items():\n",
    "    if 'tokenizer' in model_info and model_info['tokenizer'] == 'vi_tokenize':\n",
    "        model_info['tokenizer'] = vi_tokenize\n",
    "\n",
    "    all_civil_code, data_args, tfidf_vectorizer, trainer, bert_tokenizer = model_init_states[m_name]\n",
    "    \n",
    "    tokenizer = model_info.get('tokenizer')\n",
    "    topk = model_info.get('topk', 150)\n",
    "    predicted_labels, probs, c_code_pred_by_tfidf = infer_coliee_task3(\n",
    "        sentence=test_sents, \n",
    "        all_civil_code=all_civil_code,\n",
    "        data_args=data_args,\n",
    "        tfidf_vectorizer=tfidf_vectorizer,\n",
    "        trainer=trainer, bert_tokenizer=bert_tokenizer,\n",
    "        tokenizer=tokenizer, topk=topk\n",
    "    )\n",
    "\n",
    "    print('-----------------finish prediction---------------')\n",
    "    \n",
    "    predicted_labels = [x for x in list_split(predicted_labels, topk)] # np.array_split(predicted_labels, len(test_sents))\n",
    "    probs = [x for x in list_split(probs, topk)] #np.array_split(probs, len(test_sents))\n",
    "    c_code_pred_by_tfidf = [x for x in list_split(c_code_pred_by_tfidf, topk)] # np.array_split( c_code_pred_by_tfidf, len(test_sents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start infer\n",
    "time_start = time.time()\n",
    "for m_name, model_info in model_configs.items():\n",
    "    if 'tokenizer' in model_info and model_info['tokenizer'] == 'vi_tokenize':\n",
    "        model_info['tokenizer'] = vi_tokenize\n",
    "\n",
    "    all_civil_code, data_args, tfidf_vectorizer, trainer, bert_tokenizer = model_init_states[m_name]\n",
    "    \n",
    "    tokenizer = model_info.get('tokenizer')\n",
    "    topk = model_info.get('topk', 150)\n",
    "    predicted_labels, probs, c_code_pred_by_tfidf = infer_coliee_task3(\n",
    "        sentence=test_sents, \n",
    "        all_civil_code=all_civil_code,\n",
    "        data_args=data_args,\n",
    "        tfidf_vectorizer=tfidf_vectorizer,\n",
    "        trainer=trainer, bert_tokenizer=bert_tokenizer,\n",
    "        tokenizer=tokenizer, topk=topk\n",
    "    )\n",
    "\n",
    "    predicted_labels = [x for x in list_split(predicted_labels, topk)] # np.array_split(predicted_labels, len(test_sents))\n",
    "    probs = [x for x in list_split(probs, topk)] #np.array_split(probs, len(test_sents))\n",
    "    c_code_pred_by_tfidf = [x for x in list_split(c_code_pred_by_tfidf, topk)] # np.array_split( c_code_pred_by_tfidf, len(test_sents))\n",
    "\n",
    "    result = [\n",
    "        [\n",
    "            {\n",
    "                \"label\": True if lb == 1 else False,\n",
    "                \"scores\": [float(probs[jj][i][j]) for j in range(probs[jj][i].shape[0])],\n",
    "                \"id\": test_ids[jj],\n",
    "                \"sentence\": s,\n",
    "                #  \"civil_code\": c_code_pred_by_tfidf[jj][i][0],\n",
    "                \"civil_code_id\": c_code_pred_by_tfidf[jj][i][1],\n",
    "            }\n",
    "            for i, lb in enumerate(predicted_labels[jj]) if lb == 1\n",
    "        ] \n",
    "        for jj, s in enumerate(test_sents)\n",
    "    ]\n",
    "    \n",
    "    print(json.dumps(result, indent=2, ensure_ascii=False))\n",
    "    print(\"Finish inference on fine-tuned model {}, total time consuming: \".format(\n",
    "        m_name), time.time() - time_start)\n",
    "    print(len(result))\n",
    "\n",
    "print(\"Total time consuming for {} samples: {} seconds => avg 1 sample in {} second\".format(\n",
    "    len(test_sents), time.time() - time_start,  (time.time() - time_start) / len(test_sents)))\n",
    "\n",
    "submit_result = []\n",
    "for k, v in real_prediction.items():\n",
    "    relevant_a_s = []\n",
    "    for relevant_a in v:\n",
    "        tmp_a = Article.from_string(relevant_a)\n",
    "        relevant_a_s.append({'law_id': tmp_a.l_id, 'article_id': tmp_a.a_id})\n",
    "    submit_result.append({\n",
    "        'question_id': k,\n",
    "        'relevant_articles': relevant_a_s\n",
    "    })\n",
    "\n",
    "json.dump(submit_result, open(\"legal_text_retrieval/data/result_prediction.json\", \"wt\", encoding='utf8'), ensure_ascii=False, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_labels, probs, c_code_pred_by_tfidf = infer_coliee_task3(\n",
    "    sentence=test_sents, \n",
    "    all_civil_code=all_civil_code,\n",
    "    data_args=data_args,\n",
    "    tfidf_vectorizer=tfidf_vectorizer,\n",
    "    trainer=trainer, bert_tokenizer=bert_tokenizer,\n",
    "    tokenizer=tokenizer, topk=topk\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence=test_sents \n",
    "all_civil_code=all_civil_code\n",
    "data_args=data_args\n",
    "tfidf_vectorizer=tfidf_vectorizer\n",
    "trainer=trainer \n",
    "bert_tokenizer=bert_tokenizer\n",
    "tokenizer=tokenizer \n",
    "topk=topk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if isinstance(sentence, str):\n",
    "    sentence = [sentence]\n",
    "    \n",
    "    \n",
    "test_q = [Question(id='q{}'.format(i), content=tokenizer(s)\\\n",
    "    if tokenizer is not None else s, content_raw=s, relevant_a=[]) for i, s in enumerate(sentence)]\n",
    "c_docs = all_civil_code[0]\n",
    "c_keys = all_civil_code[1]\n",
    "c_vect = all_civil_code[2]\n",
    "c_docs_keys = list(zip(all_civil_code[0], all_civil_code[1]))\n",
    "\n",
    "test_pred, _ = do_classify(\n",
    "    c_docs, c_keys, test_q, vectorizer=tfidf_vectorizer, topk=topk, c_vect=c_vect,  combine_score=(tokenizer is None))\n",
    "\n",
    "c_code_pred_by_tfidf = []\n",
    "coressponding_questions = []\n",
    "for i, s_pred in enumerate(test_pred):\n",
    "    for idx in s_pred:\n",
    "        coressponding_questions.append(test_q[i].content)\n",
    "        c_code_pred_by_tfidf.append(c_docs_keys[idx])\n",
    "\n",
    "test_dataset = LawDataset(\n",
    "    data_args,\n",
    "    bert_tokenizer,\n",
    "    mode='test', \n",
    "    sentence=coressponding_questions, \n",
    "    c_code=c_code_pred_by_tfidf\n",
    ")\n",
    "\n",
    "predictions = trainer.predict(test_dataset=test_dataset).predictions\n",
    "probs = torch.softmax(torch.from_numpy(predictions), dim=1)\n",
    "predicted_labels = torch.argmax(probs, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ltr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13 (default, Oct 18 2022, 18:57:03) \n[GCC 11.2.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "482093e4637469ed0dc77f23cdb898aaa2f7d1e61448cee38f2f9cfcad0999f6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
